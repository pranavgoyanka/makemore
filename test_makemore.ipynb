{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars = ['.'] + chars\n",
    "# string to integer encoding\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "print(stoi)\n",
    "\n",
    "# integer to string encoding\n",
    "itos = {i: s for i, s in enumerate(chars)}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182543, 5]) torch.Size([182543])\n",
      "torch.Size([22787, 5]) torch.Size([22787])\n",
      "torch.Size([22816, 5]) torch.Size([22816])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "# use more than one char to predict the next char\n",
    "\n",
    "block_size = 5\n",
    "\n",
    "# the dataset would look like this\n",
    "# X[0] = [0, 0, 0, ..., 0] -> Y[0] = [4] => the first character is a itos(4) = D\n",
    "#        |--block_size---|\n",
    "# X[1] = [0, 0, 0, ..., 1] -> Y[1] = [1] => the second character is a itos(1) = A\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    \n",
    "    # end the word with a '.'\n",
    "    for ch in w + '.':\n",
    "      int_x = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(int_x)\n",
    "    \n",
    "      # remove oldest char and add latest \n",
    "      context = context[1:] + [int_x]\n",
    "  \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "  \n",
    "# split the dataset into 3 sets - train, val and test\n",
    "import random\n",
    "random.seed(123)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtrain, Ytrain = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5769, -0.3844,  0.7500,  0.0776,  0.8179,  0.4520,  0.8600,  0.0649,\n",
       "         0.1095, -0.0740])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a lookup table for each neuron that maps the 27 characters to a 10D sapce\n",
    "C = torch.randn((27, 10))\n",
    "# C is basically random data through which we will pass of our input data\n",
    "# and create it's embedded form\n",
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182543, 5]) torch.Size([27, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([182543, 5, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need our embeddings\n",
    "print(Xtrain.shape, C.shape)\n",
    "emb = C[Xtrain]\n",
    "emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182543, 50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator \n",
    "g = torch.Generator().manual_seed(12315324)\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "\n",
    "hiddenLayerSize = 150\n",
    "hiddenLayerInput = emb.shape[1] * emb.shape[2]\n",
    "numOfPossibleOutputs = 27 # 26 characters + \".\"\n",
    "\n",
    "W1 = torch.randn((hiddenLayerInput, hiddenLayerSize), generator=g)\n",
    "b1 = torch.randn(hiddenLayerSize, generator=g)\n",
    "W2 = torch.randn((hiddenLayerSize, numOfPossibleOutputs), generator=g)\n",
    "b2 = torch.randn(numOfPossibleOutputs, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11997"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182543, 50])\n",
      "torch.Size([182543, 50])\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat(torch.unbind(emb, 1), 1).shape)\n",
    "print(emb.view(-1, 50).shape)\n",
    "print(torch.cat(torch.unbind(emb, 1), 1) == emb.view(-1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.5689)\n",
      "tensor(12.3501)\n",
      "tensor(12.1487)\n",
      "tensor(11.9636)\n",
      "tensor(11.7922)\n",
      "tensor(11.6427)\n",
      "tensor(11.4865)\n",
      "tensor(11.3527)\n",
      "tensor(11.2027)\n",
      "tensor(11.0548)\n",
      "tensor(10.9313)\n",
      "tensor(10.8022)\n",
      "tensor(10.6891)\n",
      "tensor(10.5717)\n",
      "tensor(10.4568)\n",
      "tensor(10.3417)\n",
      "tensor(10.2334)\n",
      "tensor(10.1319)\n",
      "tensor(10.0410)\n",
      "tensor(9.9565)\n",
      "tensor(9.8886)\n",
      "tensor(9.8232)\n",
      "tensor(9.7942)\n",
      "tensor(9.6651)\n",
      "tensor(9.5706)\n",
      "tensor(9.4848)\n",
      "tensor(9.4091)\n",
      "tensor(9.3459)\n",
      "tensor(9.2866)\n",
      "tensor(9.2379)\n",
      "tensor(9.1678)\n",
      "tensor(9.1183)\n",
      "tensor(9.0385)\n",
      "tensor(8.9992)\n",
      "tensor(8.9109)\n",
      "tensor(8.8707)\n",
      "tensor(8.8073)\n",
      "tensor(8.7565)\n",
      "tensor(8.6874)\n",
      "tensor(8.6553)\n",
      "tensor(8.5711)\n",
      "tensor(8.5268)\n",
      "tensor(8.4757)\n",
      "tensor(8.4391)\n",
      "tensor(8.3732)\n",
      "tensor(8.3496)\n",
      "tensor(8.2725)\n",
      "tensor(8.2314)\n",
      "tensor(8.1868)\n",
      "tensor(8.1637)\n",
      "tensor(8.0976)\n",
      "tensor(8.0716)\n",
      "tensor(8.0105)\n",
      "tensor(7.9766)\n",
      "tensor(7.9374)\n",
      "tensor(7.9266)\n",
      "tensor(7.8485)\n",
      "tensor(7.8085)\n",
      "tensor(7.7768)\n",
      "tensor(7.7533)\n",
      "tensor(7.7130)\n",
      "tensor(7.6972)\n",
      "tensor(7.6305)\n",
      "tensor(7.5993)\n",
      "tensor(7.5754)\n",
      "tensor(7.5683)\n",
      "tensor(7.5253)\n",
      "tensor(7.4846)\n",
      "tensor(7.4764)\n",
      "tensor(7.4353)\n",
      "tensor(7.3850)\n",
      "tensor(7.3741)\n",
      "tensor(7.3197)\n",
      "tensor(7.2995)\n",
      "tensor(7.2749)\n",
      "tensor(7.2688)\n",
      "tensor(7.2209)\n",
      "tensor(7.1900)\n",
      "tensor(7.1835)\n",
      "tensor(7.1294)\n",
      "tensor(7.1016)\n",
      "tensor(7.0960)\n",
      "tensor(7.0398)\n",
      "tensor(7.0188)\n",
      "tensor(6.9955)\n",
      "tensor(6.9946)\n",
      "tensor(6.9420)\n",
      "tensor(6.9192)\n",
      "tensor(6.8963)\n",
      "tensor(6.8919)\n",
      "tensor(6.8536)\n",
      "tensor(6.8388)\n",
      "tensor(6.8303)\n",
      "tensor(6.7936)\n",
      "tensor(6.7758)\n",
      "tensor(6.7549)\n",
      "tensor(6.7181)\n",
      "tensor(6.7067)\n",
      "tensor(6.6755)\n",
      "tensor(6.6663)\n",
      "tensor(6.6364)\n",
      "tensor(6.6223)\n",
      "tensor(6.6077)\n",
      "tensor(6.5819)\n",
      "tensor(6.5687)\n",
      "tensor(6.5418)\n",
      "tensor(6.5189)\n",
      "tensor(6.5051)\n",
      "tensor(6.4794)\n",
      "tensor(6.4638)\n",
      "tensor(6.4455)\n",
      "tensor(6.4265)\n",
      "tensor(6.4117)\n",
      "tensor(6.3885)\n",
      "tensor(6.3731)\n",
      "tensor(6.3523)\n",
      "tensor(6.3349)\n",
      "tensor(6.3161)\n",
      "tensor(6.2994)\n",
      "tensor(6.2799)\n",
      "tensor(6.2651)\n",
      "tensor(6.2444)\n",
      "tensor(6.2304)\n",
      "tensor(6.2095)\n",
      "tensor(6.1955)\n",
      "tensor(6.1751)\n",
      "tensor(6.1611)\n",
      "tensor(6.1409)\n",
      "tensor(6.1274)\n",
      "tensor(6.1071)\n",
      "tensor(6.0940)\n",
      "tensor(6.0736)\n",
      "tensor(6.0608)\n",
      "tensor(6.0406)\n",
      "tensor(6.0279)\n",
      "tensor(6.0078)\n",
      "tensor(5.9953)\n",
      "tensor(5.9755)\n",
      "tensor(5.9631)\n",
      "tensor(5.9434)\n",
      "tensor(5.9312)\n",
      "tensor(5.9117)\n",
      "tensor(5.8996)\n",
      "tensor(5.8804)\n",
      "tensor(5.8684)\n",
      "tensor(5.8494)\n",
      "tensor(5.8376)\n",
      "tensor(5.8188)\n",
      "tensor(5.8070)\n",
      "tensor(5.7885)\n",
      "tensor(5.7768)\n",
      "tensor(5.7585)\n",
      "tensor(5.7468)\n",
      "tensor(5.7288)\n",
      "tensor(5.7171)\n",
      "tensor(5.6993)\n",
      "tensor(5.6876)\n",
      "tensor(5.6701)\n",
      "tensor(5.6583)\n",
      "tensor(5.6410)\n",
      "tensor(5.6292)\n",
      "tensor(5.6122)\n",
      "tensor(5.6002)\n",
      "tensor(5.5835)\n",
      "tensor(5.5714)\n",
      "tensor(5.5549)\n",
      "tensor(5.5428)\n",
      "tensor(5.5265)\n",
      "tensor(5.5143)\n",
      "tensor(5.4982)\n",
      "tensor(5.4860)\n",
      "tensor(5.4701)\n",
      "tensor(5.4578)\n",
      "tensor(5.4421)\n",
      "tensor(5.4298)\n",
      "tensor(5.4142)\n",
      "tensor(5.4019)\n",
      "tensor(5.3864)\n",
      "tensor(5.3743)\n",
      "tensor(5.3589)\n",
      "tensor(5.3469)\n",
      "tensor(5.3317)\n",
      "tensor(5.3199)\n",
      "tensor(5.3047)\n",
      "tensor(5.2931)\n",
      "tensor(5.2781)\n",
      "tensor(5.2667)\n",
      "tensor(5.2518)\n",
      "tensor(5.2407)\n",
      "tensor(5.2259)\n",
      "tensor(5.2150)\n",
      "tensor(5.2004)\n",
      "tensor(5.1896)\n",
      "tensor(5.1752)\n",
      "tensor(5.1646)\n",
      "tensor(5.1504)\n",
      "tensor(5.1399)\n",
      "tensor(5.1259)\n",
      "tensor(5.1156)\n",
      "tensor(5.1018)\n",
      "tensor(5.0916)\n",
      "tensor(5.0780)\n",
      "tensor(5.0679)\n",
      "tensor(5.0545)\n",
      "tensor(5.0446)\n",
      "tensor(5.0314)\n",
      "tensor(5.0216)\n",
      "tensor(5.0086)\n",
      "tensor(4.9988)\n",
      "tensor(4.9861)\n",
      "tensor(4.9764)\n",
      "tensor(4.9640)\n",
      "tensor(4.9543)\n",
      "tensor(4.9421)\n",
      "tensor(4.9324)\n",
      "tensor(4.9204)\n",
      "tensor(4.9108)\n",
      "tensor(4.8991)\n",
      "tensor(4.8895)\n",
      "tensor(4.8780)\n",
      "tensor(4.8684)\n",
      "tensor(4.8571)\n",
      "tensor(4.8476)\n",
      "tensor(4.8366)\n",
      "tensor(4.8272)\n",
      "tensor(4.8162)\n",
      "tensor(4.8071)\n",
      "tensor(4.7962)\n",
      "tensor(4.7875)\n",
      "tensor(4.7764)\n",
      "tensor(4.7684)\n",
      "tensor(4.7567)\n",
      "tensor(4.7497)\n",
      "tensor(4.7369)\n",
      "tensor(4.7309)\n",
      "tensor(4.7168)\n",
      "tensor(4.7113)\n",
      "tensor(4.6964)\n",
      "tensor(4.6911)\n",
      "tensor(4.6763)\n",
      "tensor(4.6708)\n",
      "tensor(4.6568)\n",
      "tensor(4.6510)\n",
      "tensor(4.6380)\n",
      "tensor(4.6319)\n",
      "tensor(4.6199)\n",
      "tensor(4.6134)\n",
      "tensor(4.6024)\n",
      "tensor(4.5953)\n",
      "tensor(4.5852)\n",
      "tensor(4.5774)\n",
      "tensor(4.5682)\n",
      "tensor(4.5596)\n",
      "tensor(4.5511)\n",
      "tensor(4.5417)\n",
      "tensor(4.5339)\n",
      "tensor(4.5239)\n",
      "tensor(4.5167)\n",
      "tensor(4.5063)\n",
      "tensor(4.4997)\n",
      "tensor(4.4889)\n",
      "tensor(4.4829)\n",
      "tensor(4.4719)\n",
      "tensor(4.4664)\n",
      "tensor(4.4554)\n",
      "tensor(4.4504)\n",
      "tensor(4.4397)\n",
      "tensor(4.4350)\n",
      "tensor(4.4250)\n",
      "tensor(4.4203)\n",
      "tensor(4.4109)\n",
      "tensor(4.4047)\n",
      "tensor(4.3953)\n",
      "tensor(4.3866)\n",
      "tensor(4.3774)\n",
      "tensor(4.3688)\n",
      "tensor(4.3599)\n",
      "tensor(4.3533)\n",
      "tensor(4.3442)\n",
      "tensor(4.3395)\n",
      "tensor(4.3295)\n",
      "tensor(4.3260)\n",
      "tensor(4.3150)\n",
      "tensor(4.3117)\n",
      "tensor(4.3000)\n",
      "tensor(4.2965)\n",
      "tensor(4.2847)\n",
      "tensor(4.2808)\n",
      "tensor(4.2692)\n",
      "tensor(4.2652)\n",
      "tensor(4.2537)\n",
      "tensor(4.2498)\n",
      "tensor(4.2386)\n",
      "tensor(4.2349)\n",
      "tensor(4.2237)\n",
      "tensor(4.2203)\n",
      "tensor(4.2092)\n",
      "tensor(4.2061)\n",
      "tensor(4.1950)\n",
      "tensor(4.1922)\n",
      "tensor(4.1810)\n",
      "tensor(4.1785)\n",
      "tensor(4.1674)\n",
      "tensor(4.1650)\n",
      "tensor(4.1543)\n",
      "tensor(4.1518)\n",
      "tensor(4.1417)\n",
      "tensor(4.1387)\n",
      "tensor(4.1292)\n",
      "tensor(4.1252)\n",
      "tensor(4.1161)\n",
      "tensor(4.1112)\n",
      "tensor(4.1021)\n",
      "tensor(4.0971)\n",
      "tensor(4.0881)\n",
      "tensor(4.0839)\n",
      "tensor(4.0744)\n",
      "tensor(4.0712)\n",
      "tensor(4.0612)\n",
      "tensor(4.0586)\n",
      "tensor(4.0480)\n",
      "tensor(4.0458)\n",
      "tensor(4.0347)\n",
      "tensor(4.0326)\n",
      "tensor(4.0214)\n",
      "tensor(4.0192)\n",
      "tensor(4.0082)\n",
      "tensor(4.0059)\n",
      "tensor(3.9952)\n",
      "tensor(3.9928)\n",
      "tensor(3.9825)\n",
      "tensor(3.9800)\n",
      "tensor(3.9701)\n",
      "tensor(3.9674)\n",
      "tensor(3.9580)\n",
      "tensor(3.9550)\n",
      "tensor(3.9461)\n",
      "tensor(3.9427)\n",
      "tensor(3.9343)\n",
      "tensor(3.9304)\n",
      "tensor(3.9224)\n",
      "tensor(3.9181)\n",
      "tensor(3.9103)\n",
      "tensor(3.9059)\n",
      "tensor(3.8982)\n",
      "tensor(3.8937)\n",
      "tensor(3.8860)\n",
      "tensor(3.8817)\n",
      "tensor(3.8739)\n",
      "tensor(3.8698)\n",
      "tensor(3.8620)\n",
      "tensor(3.8580)\n",
      "tensor(3.8501)\n",
      "tensor(3.8463)\n",
      "tensor(3.8383)\n",
      "tensor(3.8345)\n",
      "tensor(3.8265)\n",
      "tensor(3.8228)\n",
      "tensor(3.8149)\n",
      "tensor(3.8111)\n",
      "tensor(3.8035)\n",
      "tensor(3.7996)\n",
      "tensor(3.7922)\n",
      "tensor(3.7883)\n",
      "tensor(3.7812)\n",
      "tensor(3.7771)\n",
      "tensor(3.7703)\n",
      "tensor(3.7662)\n",
      "tensor(3.7597)\n",
      "tensor(3.7554)\n",
      "tensor(3.7492)\n",
      "tensor(3.7448)\n",
      "tensor(3.7389)\n",
      "tensor(3.7344)\n",
      "tensor(3.7287)\n",
      "tensor(3.7241)\n",
      "tensor(3.7187)\n",
      "tensor(3.7139)\n",
      "tensor(3.7087)\n",
      "tensor(3.7038)\n",
      "tensor(3.6988)\n",
      "tensor(3.6939)\n",
      "tensor(3.6890)\n",
      "tensor(3.6841)\n",
      "tensor(3.6793)\n",
      "tensor(3.6745)\n",
      "tensor(3.6696)\n",
      "tensor(3.6649)\n",
      "tensor(3.6601)\n",
      "tensor(3.6555)\n",
      "tensor(3.6506)\n",
      "tensor(3.6461)\n",
      "tensor(3.6412)\n",
      "tensor(3.6367)\n",
      "tensor(3.6318)\n",
      "tensor(3.6274)\n",
      "tensor(3.6226)\n",
      "tensor(3.6181)\n",
      "tensor(3.6134)\n",
      "tensor(3.6089)\n",
      "tensor(3.6043)\n",
      "tensor(3.5998)\n",
      "tensor(3.5954)\n",
      "tensor(3.5907)\n",
      "tensor(3.5865)\n",
      "tensor(3.5817)\n",
      "tensor(3.5777)\n",
      "tensor(3.5727)\n",
      "tensor(3.5691)\n",
      "tensor(3.5638)\n",
      "tensor(3.5605)\n",
      "tensor(3.5550)\n",
      "tensor(3.5519)\n",
      "tensor(3.5462)\n",
      "tensor(3.5434)\n",
      "tensor(3.5375)\n",
      "tensor(3.5348)\n",
      "tensor(3.5289)\n",
      "tensor(3.5263)\n",
      "tensor(3.5203)\n",
      "tensor(3.5178)\n",
      "tensor(3.5119)\n",
      "tensor(3.5092)\n",
      "tensor(3.5036)\n",
      "tensor(3.5007)\n",
      "tensor(3.4953)\n",
      "tensor(3.4921)\n",
      "tensor(3.4871)\n",
      "tensor(3.4835)\n",
      "tensor(3.4788)\n",
      "tensor(3.4749)\n",
      "tensor(3.4704)\n",
      "tensor(3.4662)\n",
      "tensor(3.4621)\n",
      "tensor(3.4576)\n",
      "tensor(3.4537)\n",
      "tensor(3.4491)\n",
      "tensor(3.4453)\n",
      "tensor(3.4406)\n",
      "tensor(3.4370)\n",
      "tensor(3.4323)\n",
      "tensor(3.4287)\n",
      "tensor(3.4240)\n",
      "tensor(3.4205)\n",
      "tensor(3.4160)\n",
      "tensor(3.4123)\n",
      "tensor(3.4081)\n",
      "tensor(3.4042)\n",
      "tensor(3.4007)\n",
      "tensor(3.3961)\n",
      "tensor(3.3942)\n",
      "tensor(3.3881)\n",
      "tensor(3.3888)\n",
      "tensor(3.3800)\n",
      "tensor(3.3837)\n",
      "tensor(3.3712)\n",
      "tensor(3.3755)\n",
      "tensor(3.3616)\n",
      "tensor(3.3647)\n",
      "tensor(3.3522)\n",
      "tensor(3.3546)\n",
      "tensor(3.3443)\n",
      "tensor(3.3471)\n",
      "tensor(3.3394)\n",
      "tensor(3.3432)\n",
      "tensor(3.3395)\n",
      "tensor(3.3407)\n",
      "tensor(3.3414)\n",
      "tensor(3.3351)\n",
      "tensor(3.3355)\n",
      "tensor(3.3311)\n",
      "tensor(3.3254)\n",
      "tensor(3.3336)\n",
      "tensor(3.3158)\n",
      "tensor(3.3278)\n",
      "tensor(3.3031)\n",
      "tensor(3.3098)\n",
      "tensor(3.2945)\n",
      "tensor(3.3015)\n",
      "tensor(3.2912)\n",
      "tensor(3.2952)\n",
      "tensor(3.2882)\n",
      "tensor(3.2878)\n",
      "tensor(3.2816)\n",
      "tensor(3.2796)\n",
      "tensor(3.2726)\n",
      "tensor(3.2727)\n",
      "tensor(3.2638)\n",
      "tensor(3.2668)\n",
      "tensor(3.2556)\n",
      "tensor(3.2603)\n",
      "tensor(3.2478)\n",
      "tensor(3.2523)\n",
      "tensor(3.2409)\n",
      "tensor(3.2447)\n",
      "tensor(3.2358)\n",
      "tensor(3.2382)\n",
      "tensor(3.2317)\n",
      "tensor(3.2319)\n",
      "tensor(3.2269)\n",
      "tensor(3.2251)\n",
      "tensor(3.2205)\n",
      "tensor(3.2184)\n",
      "tensor(3.2132)\n",
      "tensor(3.2126)\n",
      "tensor(3.2060)\n",
      "tensor(3.2078)\n",
      "tensor(3.1990)\n",
      "tensor(3.2028)\n",
      "tensor(3.1918)\n",
      "tensor(3.1958)\n",
      "tensor(3.1850)\n",
      "tensor(3.1883)\n",
      "tensor(3.1800)\n",
      "tensor(3.1824)\n",
      "tensor(3.1774)\n",
      "tensor(3.1775)\n",
      "tensor(3.1752)\n",
      "tensor(3.1718)\n",
      "tensor(3.1709)\n",
      "tensor(3.1649)\n",
      "tensor(3.1641)\n",
      "tensor(3.1580)\n",
      "tensor(3.1567)\n",
      "tensor(3.1521)\n",
      "tensor(3.1499)\n",
      "tensor(3.1488)\n",
      "tensor(3.1447)\n",
      "tensor(3.1496)\n",
      "tensor(3.1404)\n",
      "tensor(3.1472)\n",
      "tensor(3.1313)\n",
      "tensor(3.1333)\n",
      "tensor(3.1222)\n",
      "tensor(3.1241)\n",
      "tensor(3.1191)\n",
      "tensor(3.1220)\n",
      "tensor(3.1227)\n",
      "tensor(3.1224)\n",
      "tensor(3.1269)\n",
      "tensor(3.1180)\n",
      "tensor(3.1219)\n",
      "tensor(3.1098)\n",
      "tensor(3.1121)\n",
      "tensor(3.1030)\n",
      "tensor(3.1043)\n",
      "tensor(3.0996)\n",
      "tensor(3.0983)\n",
      "tensor(3.0996)\n",
      "tensor(3.0919)\n",
      "tensor(3.0966)\n",
      "tensor(3.0840)\n",
      "tensor(3.0869)\n",
      "tensor(3.0770)\n",
      "tensor(3.0783)\n",
      "tensor(3.0725)\n",
      "tensor(3.0734)\n",
      "tensor(3.0712)\n",
      "tensor(3.0715)\n",
      "tensor(3.0729)\n",
      "tensor(3.0706)\n",
      "tensor(3.0739)\n",
      "tensor(3.0676)\n",
      "tensor(3.0702)\n",
      "tensor(3.0641)\n",
      "tensor(3.0640)\n",
      "tensor(3.0633)\n",
      "tensor(3.0586)\n",
      "tensor(3.0645)\n",
      "tensor(3.0519)\n",
      "tensor(3.0572)\n",
      "tensor(3.0438)\n",
      "tensor(3.0465)\n",
      "tensor(3.0400)\n",
      "tensor(3.0415)\n",
      "tensor(3.0397)\n",
      "tensor(3.0387)\n",
      "tensor(3.0392)\n",
      "tensor(3.0349)\n",
      "tensor(3.0357)\n",
      "tensor(3.0300)\n",
      "tensor(3.0303)\n",
      "tensor(3.0250)\n",
      "tensor(3.0248)\n",
      "tensor(3.0208)\n",
      "tensor(3.0199)\n",
      "tensor(3.0178)\n",
      "tensor(3.0158)\n",
      "tensor(3.0165)\n",
      "tensor(3.0121)\n",
      "tensor(3.0155)\n",
      "tensor(3.0074)\n",
      "tensor(3.0107)\n",
      "tensor(3.0016)\n",
      "tensor(3.0034)\n",
      "tensor(2.9978)\n",
      "tensor(2.9987)\n",
      "tensor(2.9974)\n",
      "tensor(2.9967)\n",
      "tensor(2.9984)\n",
      "tensor(2.9945)\n",
      "tensor(2.9972)\n",
      "tensor(2.9905)\n",
      "tensor(2.9928)\n",
      "tensor(2.9857)\n",
      "tensor(2.9874)\n",
      "tensor(2.9816)\n",
      "tensor(2.9826)\n",
      "tensor(2.9787)\n",
      "tensor(2.9783)\n",
      "tensor(2.9766)\n",
      "tensor(2.9738)\n",
      "tensor(2.9734)\n",
      "tensor(2.9689)\n",
      "tensor(2.9686)\n",
      "tensor(2.9647)\n",
      "tensor(2.9639)\n",
      "tensor(2.9620)\n",
      "tensor(2.9607)\n",
      "tensor(2.9608)\n",
      "tensor(2.9586)\n",
      "tensor(2.9600)\n",
      "tensor(2.9566)\n",
      "tensor(2.9581)\n",
      "tensor(2.9539)\n",
      "tensor(2.9548)\n",
      "tensor(2.9514)\n",
      "tensor(2.9513)\n",
      "tensor(2.9503)\n",
      "tensor(2.9482)\n",
      "tensor(2.9506)\n",
      "tensor(2.9445)\n",
      "tensor(2.9483)\n",
      "tensor(2.9394)\n",
      "tensor(2.9418)\n",
      "tensor(2.9356)\n",
      "tensor(2.9366)\n",
      "tensor(2.9346)\n",
      "tensor(2.9339)\n",
      "tensor(2.9343)\n",
      "tensor(2.9313)\n",
      "tensor(2.9324)\n",
      "tensor(2.9278)\n",
      "tensor(2.9289)\n",
      "tensor(2.9240)\n",
      "tensor(2.9250)\n",
      "tensor(2.9205)\n",
      "tensor(2.9214)\n",
      "tensor(2.9173)\n",
      "tensor(2.9183)\n",
      "tensor(2.9145)\n",
      "tensor(2.9154)\n",
      "tensor(2.9118)\n",
      "tensor(2.9127)\n",
      "tensor(2.9093)\n",
      "tensor(2.9100)\n",
      "tensor(2.9070)\n",
      "tensor(2.9074)\n",
      "tensor(2.9049)\n",
      "tensor(2.9048)\n",
      "tensor(2.9032)\n",
      "tensor(2.9024)\n",
      "tensor(2.9020)\n",
      "tensor(2.8998)\n",
      "tensor(2.9010)\n",
      "tensor(2.8969)\n",
      "tensor(2.8986)\n",
      "tensor(2.8936)\n",
      "tensor(2.8948)\n",
      "tensor(2.8910)\n",
      "tensor(2.8913)\n",
      "tensor(2.8898)\n",
      "tensor(2.8888)\n",
      "tensor(2.8893)\n",
      "tensor(2.8866)\n",
      "tensor(2.8881)\n",
      "tensor(2.8840)\n",
      "tensor(2.8857)\n",
      "tensor(2.8812)\n",
      "tensor(2.8829)\n",
      "tensor(2.8786)\n",
      "tensor(2.8801)\n",
      "tensor(2.8762)\n",
      "tensor(2.8774)\n",
      "tensor(2.8739)\n",
      "tensor(2.8749)\n",
      "tensor(2.8717)\n",
      "tensor(2.8723)\n",
      "tensor(2.8695)\n",
      "tensor(2.8699)\n",
      "tensor(2.8672)\n",
      "tensor(2.8676)\n",
      "tensor(2.8650)\n",
      "tensor(2.8654)\n",
      "tensor(2.8628)\n",
      "tensor(2.8633)\n",
      "tensor(2.8607)\n",
      "tensor(2.8613)\n",
      "tensor(2.8586)\n",
      "tensor(2.8592)\n",
      "tensor(2.8566)\n",
      "tensor(2.8571)\n",
      "tensor(2.8548)\n",
      "tensor(2.8551)\n",
      "tensor(2.8534)\n",
      "tensor(2.8533)\n",
      "tensor(2.8526)\n",
      "tensor(2.8516)\n",
      "tensor(2.8524)\n",
      "tensor(2.8497)\n",
      "tensor(2.8517)\n",
      "tensor(2.8470)\n",
      "tensor(2.8488)\n",
      "tensor(2.8443)\n",
      "tensor(2.8452)\n",
      "tensor(2.8426)\n",
      "tensor(2.8425)\n",
      "tensor(2.8417)\n",
      "tensor(2.8404)\n",
      "tensor(2.8407)\n",
      "tensor(2.8383)\n",
      "tensor(2.8390)\n",
      "tensor(2.8361)\n",
      "tensor(2.8369)\n",
      "tensor(2.8339)\n",
      "tensor(2.8347)\n",
      "tensor(2.8318)\n",
      "tensor(2.8326)\n",
      "tensor(2.8299)\n",
      "tensor(2.8306)\n",
      "tensor(2.8280)\n",
      "tensor(2.8287)\n",
      "tensor(2.8262)\n",
      "tensor(2.8268)\n",
      "tensor(2.8244)\n",
      "tensor(2.8249)\n",
      "tensor(2.8226)\n",
      "tensor(2.8231)\n",
      "tensor(2.8209)\n",
      "tensor(2.8213)\n",
      "tensor(2.8192)\n",
      "tensor(2.8195)\n",
      "tensor(2.8176)\n",
      "tensor(2.8178)\n",
      "tensor(2.8160)\n",
      "tensor(2.8161)\n",
      "tensor(2.8146)\n",
      "tensor(2.8145)\n",
      "tensor(2.8134)\n",
      "tensor(2.8130)\n",
      "tensor(2.8126)\n",
      "tensor(2.8116)\n",
      "tensor(2.8120)\n",
      "tensor(2.8100)\n",
      "tensor(2.8110)\n",
      "tensor(2.8080)\n",
      "tensor(2.8090)\n",
      "tensor(2.8060)\n",
      "tensor(2.8066)\n",
      "tensor(2.8044)\n",
      "tensor(2.8043)\n",
      "tensor(2.8032)\n",
      "tensor(2.8024)\n",
      "tensor(2.8022)\n",
      "tensor(2.8007)\n",
      "tensor(2.8009)\n",
      "tensor(2.7989)\n",
      "tensor(2.7993)\n",
      "tensor(2.7972)\n",
      "tensor(2.7976)\n",
      "tensor(2.7955)\n",
      "tensor(2.7959)\n",
      "tensor(2.7938)\n",
      "tensor(2.7942)\n",
      "tensor(2.7922)\n",
      "tensor(2.7926)\n",
      "tensor(2.7907)\n",
      "tensor(2.7909)\n",
      "tensor(2.7892)\n",
      "tensor(2.7893)\n",
      "tensor(2.7877)\n",
      "tensor(2.7877)\n",
      "tensor(2.7863)\n",
      "tensor(2.7861)\n",
      "tensor(2.7849)\n",
      "tensor(2.7845)\n",
      "tensor(2.7836)\n",
      "tensor(2.7829)\n",
      "tensor(2.7823)\n",
      "tensor(2.7814)\n",
      "tensor(2.7810)\n",
      "tensor(2.7798)\n",
      "tensor(2.7797)\n",
      "tensor(2.7783)\n",
      "tensor(2.7784)\n",
      "tensor(2.7767)\n",
      "tensor(2.7770)\n",
      "tensor(2.7752)\n",
      "tensor(2.7756)\n",
      "tensor(2.7737)\n",
      "tensor(2.7742)\n",
      "tensor(2.7723)\n",
      "tensor(2.7727)\n",
      "tensor(2.7709)\n",
      "tensor(2.7712)\n",
      "tensor(2.7695)\n",
      "tensor(2.7697)\n",
      "tensor(2.7681)\n",
      "tensor(2.7682)\n",
      "tensor(2.7668)\n",
      "tensor(2.7667)\n",
      "tensor(2.7655)\n",
      "tensor(2.7653)\n",
      "tensor(2.7643)\n",
      "tensor(2.7641)\n",
      "tensor(2.7634)\n",
      "tensor(2.7634)\n",
      "tensor(2.7629)\n",
      "tensor(2.7637)\n",
      "tensor(2.7628)\n",
      "tensor(2.7651)\n",
      "tensor(2.7621)\n",
      "tensor(2.7650)\n",
      "tensor(2.7597)\n",
      "tensor(2.7616)\n",
      "tensor(2.7574)\n",
      "tensor(2.7582)\n",
      "tensor(2.7560)\n",
      "tensor(2.7559)\n",
      "tensor(2.7547)\n",
      "tensor(2.7540)\n",
      "tensor(2.7532)\n",
      "tensor(2.7523)\n",
      "tensor(2.7516)\n",
      "tensor(2.7506)\n",
      "tensor(2.7500)\n",
      "tensor(2.7491)\n",
      "tensor(2.7485)\n",
      "tensor(2.7477)\n",
      "tensor(2.7471)\n",
      "tensor(2.7464)\n",
      "tensor(2.7457)\n",
      "tensor(2.7451)\n",
      "tensor(2.7444)\n",
      "tensor(2.7438)\n",
      "tensor(2.7430)\n",
      "tensor(2.7425)\n",
      "tensor(2.7416)\n",
      "tensor(2.7412)\n",
      "tensor(2.7403)\n",
      "tensor(2.7398)\n",
      "tensor(2.7389)\n",
      "tensor(2.7384)\n",
      "tensor(2.7375)\n",
      "tensor(2.7370)\n",
      "tensor(2.7361)\n",
      "tensor(2.7356)\n",
      "tensor(2.7347)\n",
      "tensor(2.7342)\n",
      "tensor(2.7334)\n",
      "tensor(2.7328)\n",
      "tensor(2.7320)\n",
      "tensor(2.7314)\n",
      "tensor(2.7307)\n",
      "tensor(2.7300)\n",
      "tensor(2.7293)\n",
      "tensor(2.7286)\n",
      "tensor(2.7280)\n",
      "tensor(2.7273)\n",
      "tensor(2.7266)\n",
      "tensor(2.7259)\n",
      "tensor(2.7253)\n",
      "tensor(2.7246)\n",
      "tensor(2.7240)\n",
      "tensor(2.7233)\n",
      "tensor(2.7227)\n",
      "tensor(2.7221)\n",
      "tensor(2.7214)\n",
      "tensor(2.7208)\n",
      "tensor(2.7201)\n",
      "tensor(2.7195)\n",
      "tensor(2.7188)\n",
      "tensor(2.7183)\n",
      "tensor(2.7175)\n",
      "tensor(2.7170)\n",
      "tensor(2.7162)\n",
      "tensor(2.7157)\n",
      "tensor(2.7150)\n",
      "tensor(2.7145)\n",
      "tensor(2.7137)\n",
      "tensor(2.7132)\n",
      "tensor(2.7125)\n",
      "tensor(2.7119)\n",
      "tensor(2.7112)\n",
      "tensor(2.7107)\n",
      "tensor(2.7100)\n",
      "tensor(2.7094)\n",
      "tensor(2.7087)\n",
      "tensor(2.7082)\n",
      "tensor(2.7075)\n",
      "tensor(2.7069)\n",
      "tensor(2.7063)\n",
      "tensor(2.7057)\n",
      "tensor(2.7051)\n",
      "tensor(2.7045)\n",
      "tensor(2.7039)\n",
      "tensor(2.7033)\n",
      "tensor(2.7027)\n",
      "tensor(2.7021)\n",
      "tensor(2.7015)\n",
      "tensor(2.7010)\n",
      "tensor(2.7003)\n",
      "tensor(2.6998)\n",
      "tensor(2.6991)\n",
      "tensor(2.6986)\n",
      "tensor(2.6979)\n",
      "tensor(2.6975)\n",
      "tensor(2.6968)\n",
      "tensor(2.6963)\n",
      "tensor(2.6956)\n",
      "tensor(2.6952)\n",
      "tensor(2.6944)\n",
      "tensor(2.6940)\n",
      "tensor(2.6933)\n",
      "tensor(2.6929)\n",
      "tensor(2.6921)\n",
      "tensor(2.6917)\n",
      "tensor(2.6910)\n",
      "tensor(2.6906)\n",
      "tensor(2.6899)\n",
      "tensor(2.6894)\n",
      "tensor(2.6887)\n",
      "tensor(2.6883)\n",
      "tensor(2.6876)\n",
      "tensor(2.6872)\n",
      "tensor(2.6865)\n",
      "tensor(2.6860)\n",
      "tensor(2.6854)\n",
      "tensor(2.6849)\n",
      "tensor(2.6843)\n",
      "tensor(2.6838)\n",
      "tensor(2.6832)\n",
      "tensor(2.6827)\n",
      "tensor(2.6821)\n",
      "tensor(2.6817)\n",
      "tensor(2.6810)\n",
      "tensor(2.6806)\n",
      "tensor(2.6799)\n",
      "tensor(2.6795)\n",
      "tensor(2.6788)\n",
      "tensor(2.6785)\n",
      "tensor(2.6777)\n",
      "tensor(2.6774)\n",
      "tensor(2.6766)\n",
      "tensor(2.6763)\n",
      "tensor(2.6755)\n",
      "tensor(2.6753)\n",
      "tensor(2.6745)\n",
      "tensor(2.6742)\n",
      "tensor(2.6734)\n",
      "tensor(2.6731)\n",
      "tensor(2.6723)\n",
      "tensor(2.6721)\n",
      "tensor(2.6713)\n",
      "tensor(2.6710)\n",
      "tensor(2.6703)\n",
      "tensor(2.6700)\n",
      "tensor(2.6692)\n",
      "tensor(2.6689)\n",
      "tensor(2.6682)\n",
      "tensor(2.6679)\n",
      "tensor(2.6672)\n",
      "tensor(2.6669)\n",
      "tensor(2.6661)\n",
      "tensor(2.6658)\n",
      "tensor(2.6651)\n",
      "tensor(2.6648)\n",
      "tensor(2.6641)\n",
      "tensor(2.6638)\n",
      "tensor(2.6630)\n",
      "tensor(2.6628)\n",
      "tensor(2.6620)\n",
      "tensor(2.6618)\n",
      "tensor(2.6610)\n",
      "tensor(2.6608)\n",
      "tensor(2.6600)\n",
      "tensor(2.6598)\n",
      "tensor(2.6589)\n",
      "tensor(2.6588)\n",
      "tensor(2.6579)\n",
      "tensor(2.6578)\n",
      "tensor(2.6569)\n",
      "tensor(2.6568)\n",
      "tensor(2.6559)\n",
      "tensor(2.6558)\n",
      "tensor(2.6550)\n",
      "tensor(2.6548)\n",
      "tensor(2.6540)\n",
      "tensor(2.6538)\n",
      "tensor(2.6531)\n",
      "tensor(2.6529)\n",
      "tensor(2.6521)\n",
      "tensor(2.6519)\n",
      "tensor(2.6512)\n",
      "tensor(2.6510)\n",
      "tensor(2.6502)\n",
      "tensor(2.6500)\n",
      "tensor(2.6493)\n",
      "tensor(2.6491)\n",
      "tensor(2.6483)\n",
      "tensor(2.6482)\n",
      "tensor(2.6474)\n",
      "tensor(2.6473)\n",
      "tensor(2.6464)\n",
      "tensor(2.6463)\n",
      "tensor(2.6454)\n",
      "tensor(2.6454)\n",
      "tensor(2.6445)\n",
      "tensor(2.6445)\n",
      "tensor(2.6435)\n",
      "tensor(2.6435)\n",
      "tensor(2.6426)\n",
      "tensor(2.6426)\n",
      "tensor(2.6416)\n",
      "tensor(2.6417)\n",
      "tensor(2.6407)\n",
      "tensor(2.6408)\n",
      "tensor(2.6399)\n",
      "tensor(2.6399)\n",
      "tensor(2.6390)\n",
      "tensor(2.6390)\n",
      "tensor(2.6382)\n",
      "tensor(2.6381)\n",
      "tensor(2.6374)\n",
      "tensor(2.6373)\n",
      "tensor(2.6366)\n",
      "tensor(2.6365)\n",
      "tensor(2.6357)\n",
      "tensor(2.6356)\n",
      "tensor(2.6349)\n",
      "tensor(2.6348)\n",
      "tensor(2.6340)\n",
      "tensor(2.6340)\n",
      "tensor(2.6332)\n",
      "tensor(2.6332)\n",
      "tensor(2.6323)\n",
      "tensor(2.6323)\n",
      "tensor(2.6314)\n",
      "tensor(2.6315)\n",
      "tensor(2.6305)\n",
      "tensor(2.6306)\n",
      "tensor(2.6296)\n",
      "tensor(2.6298)\n",
      "tensor(2.6287)\n",
      "tensor(2.6289)\n",
      "tensor(2.6278)\n",
      "tensor(2.6280)\n",
      "tensor(2.6270)\n",
      "tensor(2.6272)\n",
      "tensor(2.6262)\n",
      "tensor(2.6264)\n",
      "tensor(2.6255)\n",
      "tensor(2.6256)\n",
      "tensor(2.6248)\n",
      "tensor(2.6249)\n",
      "tensor(2.6242)\n",
      "tensor(2.6242)\n",
      "tensor(2.6235)\n",
      "tensor(2.6235)\n",
      "tensor(2.6228)\n",
      "tensor(2.6227)\n",
      "tensor(2.6221)\n",
      "tensor(2.6220)\n",
      "tensor(2.6214)\n",
      "tensor(2.6213)\n",
      "tensor(2.6207)\n",
      "tensor(2.6206)\n",
      "tensor(2.6199)\n",
      "tensor(2.6199)\n",
      "tensor(2.6191)\n",
      "tensor(2.6191)\n",
      "tensor(2.6182)\n",
      "tensor(2.6184)\n",
      "tensor(2.6173)\n",
      "tensor(2.6176)\n",
      "tensor(2.6164)\n",
      "tensor(2.6168)\n",
      "tensor(2.6154)\n",
      "tensor(2.6158)\n",
      "tensor(2.6144)\n",
      "tensor(2.6149)\n",
      "tensor(2.6135)\n",
      "tensor(2.6140)\n",
      "tensor(2.6128)\n",
      "tensor(2.6132)\n",
      "tensor(2.6122)\n",
      "tensor(2.6125)\n",
      "tensor(2.6117)\n",
      "tensor(2.6119)\n",
      "tensor(2.6113)\n",
      "tensor(2.6114)\n",
      "tensor(2.6110)\n",
      "tensor(2.6110)\n",
      "tensor(2.6106)\n",
      "tensor(2.6105)\n",
      "tensor(2.6102)\n",
      "tensor(2.6100)\n",
      "tensor(2.6097)\n",
      "tensor(2.6094)\n",
      "tensor(2.6091)\n",
      "tensor(2.6087)\n",
      "tensor(2.6085)\n",
      "tensor(2.6080)\n",
      "tensor(2.6078)\n",
      "tensor(2.6073)\n",
      "tensor(2.6070)\n",
      "tensor(2.6065)\n",
      "tensor(2.6062)\n",
      "tensor(2.6058)\n",
      "tensor(2.6055)\n",
      "tensor(2.6051)\n",
      "tensor(2.6048)\n",
      "tensor(2.6045)\n",
      "tensor(2.6042)\n",
      "tensor(2.6043)\n",
      "tensor(2.6036)\n",
      "tensor(2.6044)\n",
      "tensor(2.6027)\n",
      "tensor(2.6045)\n",
      "tensor(2.6010)\n",
      "tensor(2.6029)\n",
      "tensor(2.5984)\n",
      "tensor(2.5999)\n",
      "tensor(2.5961)\n",
      "tensor(2.5974)\n",
      "tensor(2.5945)\n",
      "tensor(2.5957)\n",
      "tensor(2.5933)\n",
      "tensor(2.5946)\n",
      "tensor(2.5925)\n",
      "tensor(2.5938)\n",
      "tensor(2.5917)\n",
      "tensor(2.5931)\n",
      "tensor(2.5910)\n",
      "tensor(2.5924)\n",
      "tensor(2.5903)\n",
      "tensor(2.5917)\n",
      "tensor(2.5897)\n",
      "tensor(2.5910)\n",
      "tensor(2.5891)\n",
      "tensor(2.5904)\n",
      "tensor(2.5886)\n",
      "tensor(2.5899)\n",
      "tensor(2.5883)\n",
      "tensor(2.5896)\n",
      "tensor(2.5883)\n",
      "tensor(2.5895)\n",
      "tensor(2.5889)\n",
      "tensor(2.5901)\n",
      "tensor(2.5903)\n",
      "tensor(2.5916)\n",
      "tensor(2.5929)\n",
      "tensor(2.5941)\n",
      "tensor(2.5960)\n",
      "tensor(2.5971)\n",
      "tensor(2.5985)\n",
      "tensor(2.5999)\n",
      "tensor(2.5996)\n",
      "tensor(2.6041)\n",
      "tensor(2.6023)\n",
      "tensor(2.6119)\n",
      "tensor(2.6047)\n",
      "tensor(2.6062)\n",
      "tensor(2.5961)\n",
      "tensor(2.5978)\n",
      "tensor(2.5927)\n",
      "tensor(2.5955)\n",
      "tensor(2.5913)\n",
      "tensor(2.5945)\n",
      "tensor(2.5904)\n",
      "tensor(2.5936)\n",
      "tensor(2.5894)\n",
      "tensor(2.5926)\n",
      "tensor(2.5884)\n",
      "tensor(2.5914)\n",
      "tensor(2.5875)\n",
      "tensor(2.5905)\n",
      "tensor(2.5867)\n",
      "tensor(2.5896)\n",
      "tensor(2.5860)\n",
      "tensor(2.5889)\n",
      "tensor(2.5854)\n",
      "tensor(2.5882)\n",
      "tensor(2.5847)\n",
      "tensor(2.5876)\n",
      "tensor(2.5841)\n",
      "tensor(2.5869)\n",
      "tensor(2.5835)\n",
      "tensor(2.5862)\n",
      "tensor(2.5828)\n",
      "tensor(2.5856)\n",
      "tensor(2.5822)\n",
      "tensor(2.5849)\n",
      "tensor(2.5816)\n",
      "tensor(2.5842)\n",
      "tensor(2.5809)\n",
      "tensor(2.5836)\n",
      "tensor(2.5803)\n",
      "tensor(2.5829)\n",
      "tensor(2.5797)\n",
      "tensor(2.5823)\n",
      "tensor(2.5791)\n",
      "tensor(2.5816)\n",
      "tensor(2.5785)\n",
      "tensor(2.5810)\n",
      "tensor(2.5779)\n",
      "tensor(2.5804)\n",
      "tensor(2.5774)\n",
      "tensor(2.5798)\n",
      "tensor(2.5768)\n",
      "tensor(2.5792)\n",
      "tensor(2.5762)\n",
      "tensor(2.5786)\n",
      "tensor(2.5756)\n",
      "tensor(2.5780)\n",
      "tensor(2.5751)\n",
      "tensor(2.5774)\n",
      "tensor(2.5745)\n",
      "tensor(2.5768)\n",
      "tensor(2.5740)\n",
      "tensor(2.5763)\n",
      "tensor(2.5734)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     13\u001b[0m   p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iters = 100000\n",
    "for i in range(iters):\n",
    "  # forward pass\n",
    "  emb = C[Xtrain]\n",
    "  h = torch.tanh(emb.view(-1, 50) @ W1 + b1)\n",
    "  logits = h @ W2 + b2\n",
    "\n",
    "  # loss\n",
    "  loss = F.cross_entropy(logits, Ytrain)\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights\n",
    "  for p in parameters:\n",
    "    p.data += -0.1 * p.grad\n",
    "\n",
    "  print(loss.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "careah.\n",
      "aablil.\n",
      "khkimri.\n",
      "rehty.\n",
      "sacanane.\n",
      "rahnen.\n",
      "deliyha.\n",
      "kaeei.\n",
      "nerania.\n",
      "ceriir.\n",
      "kalenn.\n",
      "dhlmo.\n",
      "diniqhann.\n",
      "salin.\n",
      "alian.\n",
      "qurrae.\n",
      "maijaryni.\n",
      "jaceininsa.\n",
      "medde.\n",
      "iia.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "  out = []\n",
    "  ctx = [0] * block_size\n",
    "  \n",
    "  while True:\n",
    "    emb = C[torch.tensor([ctx])]\n",
    "    h = torch.tanh(emb.view(-1, 50) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "    ctx = ctx[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "    \n",
    "  print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits=tensor([[  3.3956,   6.5123,  -3.1138, -10.4414,  -6.2226,   5.0934,  -6.2482,\n",
      "         -14.9007,   5.8073,  -0.3638,   4.4702,   0.3045,   5.6061,   5.8509,\n",
      "           5.9202,   5.0284,  -4.1170,  -2.9448,   5.5049,   7.5590,  -8.1065,\n",
      "          -0.1808,   3.7585,  -8.1800,  -7.2704,   2.1393,   9.5375]],\n",
      "       grad_fn=<AddBackward0>) logits.shape=torch.Size([1, 27])\n",
      "probs=tensor([[1.6109e-03, 3.6360e-02, 2.3992e-06, 1.5766e-09, 1.0713e-07, 8.7982e-03,\n",
      "         1.0443e-07, 1.8242e-11, 1.7967e-02, 3.7529e-05, 4.7180e-03, 7.3215e-05,\n",
      "         1.4692e-02, 1.8766e-02, 2.0114e-02, 8.2450e-03, 8.7980e-07, 2.8409e-06,\n",
      "         1.3278e-02, 1.0357e-01, 1.6284e-08, 4.5067e-05, 2.3157e-03, 1.5130e-08,\n",
      "         3.7573e-08, 4.5861e-04, 7.4895e-01]], grad_fn=<SoftmaxBackward0>) probs.shape=torch.Size([1, 27])\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# ctx = [0] * block_size\n",
    "# out = []\n",
    "\n",
    "emb = C[torch.tensor([ctx])]\n",
    "h = torch.tanh(emb.view(-1, 50) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "print(f'{logits=} {logits.shape=}')\n",
    "probs = F.softmax(logits, dim=1)\n",
    "print(f'{probs=} {probs.shape=}')\n",
    "ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "print(f'{ix}')\n",
    "ctx = ctx[1:] + [ix]\n",
    "out.append(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalaras.ipvaz\n"
     ]
    }
   ],
   "source": [
    "out\n",
    "print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
